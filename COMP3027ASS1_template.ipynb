{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Leaner for Adult Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name(s):** Hanyue Zhu\n",
    "<br>\n",
    "**Student ID(s):** 1174426"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: Friday, 8 April 2022 7pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: This iPython notebook is a template which you will use for your Assignment 1 submission. You need to only submitted the completed copy of this iPython notebook.\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count). Submissions more than 5 days late will not be accepted (resul in a mark of 0).\n",
    "<ul>\n",
    "    <li>one day late, -1.0;</li>\n",
    "    <li>two days late, -2.0;</li>\n",
    "    <li>three days late, -3.0;</li>\n",
    "    <li>four days late, -4.0;</li>\n",
    "    <li>five days late, -5.0;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Extensions</b>: Students who are demonstrably unable to submit a full solution in time due to medical reasons or other trauma, may apply for an extension.  In these cases, you should email <a href=\"mailto:ni.ding@unimelb.edu.au\">Ni Ding</a> as soon as possible after those circumstances arise. If you attend a GP or other health care service as a result of illness, be sure to provide a Health Professional Report (HPR) form (get it from the Special Consideration section of the Student Portal), you will need this form to be filled out if your illness develops into something that later requires a Special Consideration application to be lodged. You should scan the HPR form and send it with the extension requests.\n",
    "\n",
    "<b>Marks</b>: This assignment will be marked out of 20, and make up 20% of your overall mark for this subject.\n",
    "\n",
    "<b>Materials</b>: Use Jupyter Notebook and Python page on Canvas for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn. You can use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. We reserve the right to deduct up to 2 marks for unreadable or exessively inefficient code.\n",
    "\n",
    "8 of the marks available for this Project will be assigned to whether the four specified Python functions work in a manner consistent with the materials from COMP30027. Any other implementation will not be directly assessed (except insofar as it is required to make these five functions work correctly).\n",
    "\n",
    "12 of the marks will be assigned to your responses to the questions, in terms of both accuracy and insightfulness. We will be looking for evidence that you have an implementation that allows you to explore the problem, but also that you have thought deeply about the data and the behaviour of the Naive Bayes classifier.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board (ED -> Assignments -> A1); we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/124196/modules#module_662096\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>missing Authorship Declaration at the bottom of the page, -5.0\n",
    "<LI>incomplete or unsigned Authorship Declaration at the bottom of the page, -3.0\n",
    "</UL>\n",
    "**NOTE: COMPLETE AND SUBMIT THIS FILE. YOU SHOULD IMPLEMENT FOUR FUNCTIONS AND INCLUDE YOUR ANSWERS TO THE QUESTIONS IN THIS FILE ONLY. NO OTHER SUBMISSION IS REQUIRED.**\n",
    "\n",
    "**Keep your code clean. Adding proper comments to your code is MANDATORY.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Base code [8 marks]\n",
    "\n",
    "Instructions\n",
    "1. Do **not** shuffle the data set\n",
    "2. Treat the attributes as they are(e.g., do **not** convert numeric attributes to categorical or categorical to numeric). Implement a Naive Bayes classifier with appropriate likelihood function for each attribute.\n",
    "3. You should implement the Naive Bayes classifier from scratch. Do **not** use existing implementations/learning algorithms.\n",
    "4. You CANNOT have more than one train or predict function. Both continuous numeric attributes and categorical ones should be trained in one `train()` function, similarly for the `predict()`.  \n",
    "5. Apart from the instructions in point 3, you may use libraries to help you with data reading, representation, maths or evaluation\n",
    "6. Ensure that all and only required information is printed, as indicated in the final three code cells. Failure to adhere to print the required information will result in **[-1 mark]** per case. *(We don't mind details like you print a list or several numbers -- just make sure the information is displayed so that it's easily accessible)\n",
    "7. You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. \n",
    "8. You should add adequate comments to make your code easily comprehendible.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "# and implement 90-10 splitting as specified in the project description.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def preprocess(filename):\n",
    "    attr = []\n",
    "    label = []\n",
    "    total_ins = []\n",
    "    with open(filename,\"r\") as f:#Open csv data by using open\n",
    "        for i in f.readlines()[1:]:\n",
    "            total_ins.append(i.strip().split(\",\"))#put the data in total_ins list\n",
    "            attr.append(i.strip().split(\",\")[:-1])#put the attributes and its values in attr list\n",
    "            label.append(i.strip().split(\",\")[-1])#put labels in label list\n",
    "    f.close()\n",
    "    \n",
    "    f_len = len(attr)#find the total instances\n",
    "    ratio = 0.9 #90-10 splitting\n",
    "    n = int(ratio*f_len)\n",
    "    #assign instances into x_train,x_test,y_train,y_test\n",
    "    x_train = attr[:n]\n",
    "    y_train = label[:n]\n",
    "    x_test = attr[n:f_len]\n",
    "    y_test = label[n:f_len]\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test,total_ins,attr,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculat prior probabilities and likelihoods (conditional probabilities) from the training data and using\n",
    "# to build a naive Bayes model\n",
    "#find the index of numeric and nominal attributes\n",
    "def get_num_and_nom_index(f):\n",
    "    num_index = []\n",
    "    nom_index = []\n",
    "    a = []\n",
    "    b = []\n",
    "    for i in range(len(f)):\n",
    "        for j in range(len(f[i])):\n",
    "            if ((len(f[i][j]) == 1 or len(f[i][j]) == 2) and f[i][j] != \" ?\"):\n",
    "                a.append(j)\n",
    "            else:\n",
    "                b.append(j)\n",
    "                \n",
    "    for k in set(a):\n",
    "        num_index.append(k)\n",
    "    for h in set(b):\n",
    "        nom_index.append(h)\n",
    "        \n",
    "    return sorted(num_index),sorted(nom_index)\n",
    "\n",
    "#find the index of the column of label\n",
    "def get_label_index(f):\n",
    "    c1_index = [] #the list used to store index of label  \"<=50K\"\n",
    "    c2_index = []#list used to store index of label \">50K\"\n",
    "    for i in range(len(f)):\n",
    "        if (f[i] == \" <=50K\"):\n",
    "            c1_index.append(i)\n",
    "        else:\n",
    "            c2_index.append(i)\n",
    "    \n",
    "    return c1_index,c2_index\n",
    "\n",
    "#create the formula of standard deviation \n",
    "def sd(mean,attr):\n",
    "    su = 0\n",
    "    for i in attr:\n",
    "        su += (i-mean)**2/(len(attr)-1)\n",
    "    return np.sqrt(su)\n",
    "\n",
    "#find the formula of gaussian pdf \n",
    "def gaussian_pdf(mean, sd, x):\n",
    "    return np.log(np.exp((-1/2)*np.square((x-mean)/sd))/(sd*np.sqrt(2*np.pi)))\n",
    "\n",
    "#a function that helps to find the mean and sd of numeric attributes\n",
    "def get_mean_sd(x_train,y_train,num_index):\n",
    "    c1_index,c2_index = get_label_index(y_train)\n",
    "    \n",
    "    c1_mean_list = []\n",
    "    c2_mean_list = []\n",
    "    c1_sd_list = []\n",
    "    c2_sd_list = []\n",
    "    for i in num_index:\n",
    "        c1_attr = []\n",
    "        c2_attr = []\n",
    "        c1_mean = 0\n",
    "        c1_sd = 0\n",
    "        c2_mean = 0\n",
    "        c2_sd = 0\n",
    "        for j in c1_index:\n",
    "            c1_attr.append(int(x_train[j][i]))#numeric attributes with label c1(\" <=50K\")\n",
    "        c1_mean = np.mean(c1_attr)\n",
    "        c1_sd = sd(c1_mean,c1_attr)\n",
    "        for k in c2_index:\n",
    "            c2_attr.append(int(x_train[k][i]))#numeric attributes with label c2\n",
    "        c2_mean = np.mean(c2_attr)\n",
    "        c2_sd = sd(c2_mean,c2_attr)\n",
    "        \n",
    "        #use lists to store means and sd\n",
    "        c1_mean_list.append(c1_mean)\n",
    "        c2_mean_list.append(c2_mean)\n",
    "        c1_sd_list.append(c1_sd)\n",
    "        c2_sd_list.append(c2_sd)\n",
    "    \n",
    "    c1_list = []\n",
    "    c2_list = []\n",
    "    c1_list.append((c1_mean_list,c1_sd_list))\n",
    "    c2_list.append((c2_mean_list,c2_sd_list))\n",
    "                \n",
    "    return c1_list,c2_list\n",
    "\n",
    "# find the attribute values of each attributes\n",
    "def get_attr_freq(x_train,nom_index):\n",
    "    \n",
    "    attr_list = []\n",
    "    for i in nom_index: \n",
    "        attr_value = []\n",
    "\n",
    "        for j in range(len(x_train)):\n",
    "            attr_value.append(x_train[j][i])\n",
    "            \n",
    "        attr_list.append(list(set(attr_value)))#use a list to store the attribute values\n",
    "        \n",
    "    return attr_list\n",
    "\n",
    "#find the likelihood of each \n",
    "def get_likelihood(x_train,y_train,nom_index):\n",
    "        \n",
    "    c1_index,c2_index = get_label_index(y_train)\n",
    "    attr_list = get_attr_freq(x_train,nom_index)\n",
    "        \n",
    "    c1_lld_list = []\n",
    "    c2_lld_list = []\n",
    "    parameter_list = []\n",
    "    for i in range(len(nom_index)):\n",
    "        a = []\n",
    "        b = []\n",
    "        c = []\n",
    "        d = []\n",
    "        n = []\n",
    "        m = []\n",
    "        for j in c1_index:\n",
    "            a.append(x_train[j][nom_index[i]])#list a is used to collect the nominal attributes instances with label c1\n",
    "        for p in c2_index:\n",
    "            b.append(x_train[p][nom_index[i]])#list a is used to collect the nominal attributes instances with label c2\n",
    "        for k in attr_list[i]:\n",
    "            count1 = 0\n",
    "            for p in a:  # count the frequences of attribute value of each nominal attributes with label c1\n",
    "                if (k==p):\n",
    "                    count1 += 1\n",
    "            c.append(count1)\n",
    "        for s in c:# add 1 laplace smoothing is used to deal with 0 probability\n",
    "            d.append(np.log((s+1)/(len(c1_index)+len(attr_list[i]))))\n",
    "        c1_lld_list.append(d)\n",
    "        \n",
    "        for f in attr_list[i]:\n",
    "            count2 = 0\n",
    "            for s in b: # count the frequences of attribute value of each nominal attributes with label c2\n",
    "                if (f==s):\n",
    "                    count2 += 1\n",
    "            n.append(count2)\n",
    "        for e in n:# add 1 laplace smoothing is used to deal with 0 probability\n",
    "            m.append(np.log((e+1)/(len(c2_index)+len(attr_list[i]))))\n",
    "        c2_lld_list.append(m)\n",
    "    \n",
    "    return c1_lld_list,c2_lld_list,attr_list\n",
    "\n",
    "\n",
    "def train(x_train,y_train):\n",
    "    c1_index,c2_index = get_label_index(y_train)\n",
    "    prior = []\n",
    "    c1_prior = np.log(len(c1_index)/len(y_train))\n",
    "    c2_prior = np.log(len(c2_index)/len(y_train))\n",
    "    prior.append((c1_prior,c2_prior))\n",
    "    \n",
    "    num_index,nom_index=get_num_and_nom_index(x_train)\n",
    "    \n",
    "    c1_list,c2_list = get_mean_sd(x_train,y_train,num_index)\n",
    "    c1_lld_list,c2_lld_list,attr_list = get_likelihood(x_train,y_train,nom_index)\n",
    "\n",
    "    return prior,c1_list,c2_list,c1_lld_list,c2_lld_list\n",
    "\n",
    "# a function used to find the sum guassian likelihood of each numeric instance\n",
    "def sum_gaussian_pdf(test_data,c1_list,c2_list):\n",
    "    \n",
    "    num_test_index,nom_test_index = get_num_and_nom_index(test_data)#columns\n",
    "    \n",
    "    c1_lld = 0\n",
    "    c2_lld = 0\n",
    "    c1_test_list = [] \n",
    "    c2_test_list = []\n",
    "    attr_value1 = 0\n",
    "    for i in range(len(test_data)):\n",
    "        c1_lld = 0\n",
    "        for j in range(len(num_test_index)):\n",
    "            mean1 = c1_list[0][0][j]\n",
    "            sd1 = c1_list[0][1][j]\n",
    "            #find the sum gaussian likelihood of numeric instances with label c1\n",
    "            c1_lld += gaussian_pdf(mean1,sd1,int(test_data[i][num_test_index[j]]))\n",
    "            \n",
    "        c1_test_list.append(c1_lld)\n",
    "    for i in range(len(test_data)):\n",
    "        c2_lld = 0\n",
    "        for j in range(len(num_test_index)):\n",
    "            mean2 = c2_list[0][0][j]\n",
    "            sd2 = c2_list[0][1][j]\n",
    "            #find the sum gaussian likelihood of numeric instances with label c2\n",
    "            c2_lld += gaussian_pdf(mean2,sd2,int(test_data[i][num_test_index[j]]))\n",
    "            \n",
    "        c2_test_list.append(c2_lld)\n",
    "        \n",
    "    return c1_test_list,c2_test_list\n",
    "\n",
    "# a function used to find the sum likelihood of each nominal instance\n",
    "def sum_likelihood(test_data,c1_lld_list,c2_lld_list):\n",
    "    \n",
    "    #find the numeric index and nominal index of test data\n",
    "    num_test_index,nom_test_index = get_num_and_nom_index(test_data)\n",
    "    c1_lld_list,c2_lld_list,attr_list = get_likelihood(x_train,y_train,nom_index)\n",
    "    \n",
    "    c1_lld = 0\n",
    "    c2_lld = 0\n",
    "    c1 = []\n",
    "    c2 = []\n",
    "    a = []\n",
    "    b = []\n",
    "    c1_nom_list = []\n",
    "    c2_nom_list = []\n",
    "  \n",
    "    for i in range(len(test_data)): \n",
    "        c1_lld = 0\n",
    "        c2_lld = 0\n",
    "        for j in range(len(nom_test_index)):\n",
    "            r = test_data[i][nom_test_index[j]]#the nominal attribute value of each instance of the test data\n",
    "            if (r in attr_list[j]):#check whether the attribute value of test data is exist in x train data\n",
    "                h = attr_list[j].index(r)\n",
    "                c1_lld += c1_lld_list[j][h]\n",
    "                c2_lld += c2_lld_list[j][h]\n",
    "        #find the likelihood of each nominal instance of test data and store them in lists\n",
    "        c1_nom_list.append(c1_lld)\n",
    "        c2_nom_list.append(c2_lld)\n",
    "        \n",
    "    return c1_nom_list,c2_nom_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in the testing data\n",
    "def predict(x_test,y_train):\n",
    "    #a list used to store the predicted label of x_test\n",
    "    predicted_label = []\n",
    "    c_hat = []#a list used to collect the higher posterior of each instance of x_test\n",
    "    #find the log prior probability of c1 and c2\n",
    "    c1_prior = prior[0][0]\n",
    "    c2_prior = prior[0][1]\n",
    "    \n",
    "    #find c1 sum gaussian likelihood and c2 sum gaussian likelihood\n",
    "    c1_numeric_lld,c2_numeric_lld = sum_gaussian_pdf(x_test,c1_list,c2_list)\n",
    "    #find c1 sum likelihood and c2 sum likelihood\n",
    "    c1_nominal_lld,c2_nominal_lld = sum_likelihood(x_test,c1_lld_list,c2_lld_list)\n",
    "    \n",
    "    #use 2 lists to store the posterior probabilities of c1 and c2\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        c1_posterior = c1_prior+c1_numeric_lld[i]+c1_nominal_lld[i]\n",
    "        list1.append(c1_posterior)\n",
    "        c2_posterior = c2_prior+c2_numeric_lld[i]+c2_nominal_lld[i]\n",
    "        list2.append(c2_posterior)\n",
    "    for j in range(len(x_test)):\n",
    "        #compare the c1 posterior and c2 posterior ,find the higher posterior and store its label in predicted_label list\n",
    "        if (list1[j] < list2[j]):\n",
    "            predicted_label.append(\" >50K\")\n",
    "        else:\n",
    "            predicted_label.append(\" <=50K\")\n",
    "            \n",
    "        #use \"max\" to find the higher posterior and collect it in list c_hat\n",
    "        c_hat.append(max(list1[j],list2[j]))\n",
    "        \n",
    "    \n",
    "    return predicted_label,c_hat,list1,list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels, return and output accuracy, confusion matrix and F1 score.\n",
    "\n",
    "def evaluate(predicted_label,y_test):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    #find the TP,TN,FP,FN\n",
    "    for i in range(len(y_test)):\n",
    "        if (predicted_label[i]==\" <=50K\" and y_test[i]==\" <=50K\"):\n",
    "            TP += 1\n",
    "        elif (predicted_label[i]==\" >50K\" and y_test[i]==\" >50K\"):\n",
    "            TN += 1\n",
    "        elif (predicted_label[i]==\" <=50K\" and y_test[i]==\" >50K\"):\n",
    "            FP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "    #find the accuracy and confusion and f-score\n",
    "    accuracy = (TP+TN)/len(y_test)\n",
    "    k = []\n",
    "    p = []\n",
    "    t = []\n",
    "    k.append(TP)\n",
    "    k.append(FN)\n",
    "    p.append(FP)\n",
    "    p.append(TN)\n",
    "    t.append(k)\n",
    "    t.append(p)\n",
    "    matrix = t\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    f1 = (2*precision*recall)/(precision+recall)\n",
    "    \n",
    "    return accuracy,matrix,f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.86\n",
      "f1-score:  0.9078947368421053\n",
      "confusion matrix:  [[69, 8], [6, 17]]\n",
      "                   \n",
      "Attribute vectors of instances [0, 1, 2]:  [['68', ' ?', ' 1st-4th', '2', ' Divorced', ' ?', ' Not-in-family', ' White', ' Female', '20', ' United-States'], ['39', ' State-gov', ' Bachelors', '13', ' Never-married', ' Adm-clerical', ' Not-in-family', ' White', ' Male', '40', ' United-States'], ['50', ' Self-emp-not-inc', ' Bachelors', '13', ' Married-civ-spouse', ' Exec-managerial', ' Husband', ' White', ' Male', '13', ' United-States']]\n",
      "\n",
      "Number of instances (N):  1000\n",
      "Number of attributes (F):  11\n",
      "Number of labels (L):  2\n",
      "\n",
      "\n",
      "Predicted class log-probabilities for instance N-3: <=50K: -20.768165488458923 ,   >50K: -19.762177925498445\n",
      "Predicted class ID for instance N-3:   >50K\n",
      "\n",
      "Predicted class log-probabilities for instance N-2: <=50K: -25.310387321914632 ,   >50K: -22.803496285083618\n",
      "Predicted class ID for instance N-2:   >50K\n",
      "\n",
      "Predicted class log-probabilities for instance N-1: <=50K: -16.929982096849525 ,   >50K: -16.91915916100011\n",
      "Predicted class ID for instance N-1:   >50K\n"
     ]
    }
   ],
   "source": [
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full ADULT data set, and print the evaluation results. [0.33 marks]\n",
    "\n",
    "\n",
    "\n",
    "# First, read in the data and apply your NB model to the ADULT data\n",
    "x_train,x_test,y_train,y_test,total_ins,attr,label= preprocess(\"adult.csv\")\n",
    "num_index,nom_index = get_num_and_nom_index(x_train)\n",
    "c1_index,c2_index = get_label_index(y_train)\n",
    "prior,c1_list,c2_list,c1_lld_list,c2_lld_list = train(x_train,y_train)\n",
    "attr_list = get_attr_freq(x_train,nom_index)\n",
    "predicted_label,c_hat,list1,list2= predict(x_test,y_train)\n",
    "\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "accuracy,matrix,f1 = evaluate(predicted_label,y_test)\n",
    "print(\"accuracy: \",accuracy)\n",
    "print(\"f1-score: \",f1)\n",
    "print(\"confusion matrix: \",matrix)\n",
    "print(\"                   \")\n",
    "\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of attributes, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "# The \"predicted class ID\" must be in range (0, L)\n",
    "\n",
    "print(\"Attribute vectors of instances [0, 1, 2]: \", x_train[0:3]) # of the first three records in adult.csv\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", len(total_ins))\n",
    "print(\"Number of attributes (F): \", len(attr[0]))\n",
    "print(\"Number of labels (L): \", len(set(label)))\n",
    "\n",
    "\n",
    "# print out the prediction results of the last three instances\n",
    "print(\"\\n\\nPredicted class log-probabilities for instance N-3: <=50K:\",list1[-3],\", \",\" >50K:\",list2[-3])\n",
    "print(\"Predicted class ID for instance N-3: \", predicted_label[-3])\n",
    "print(\"\\nPredicted class log-probabilities for instance N-2: <=50K:\",list1[-2],\", \",\" >50K:\",list2[-2])\n",
    "print(\"Predicted class ID for instance N-2: \", predicted_label[-2])\n",
    "print(\"\\nPredicted class log-probabilities for instance N-1: <=50K:\",list1[-1],\", \",\" >50K:\",list2[-1])\n",
    "print(\"Predicted class ID for instance N-1: \", predicted_label[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Conceptual questions [8 marks for groups of 1] / [16 marks for groups of 2]\n",
    "\n",
    "\n",
    "If you are in a group of 1, you should respond to Q1 and Q2.\n",
    "\n",
    "If you are in a group of 2, you should respond to Q1, Q2, Q3 and Q4.\n",
    "\n",
    "A response to a question should take about 100–250 words. You may need to develope codes or functions to help respond to the question here. \n",
    "\n",
    "#### NOTE: We strongly recommend <u>including figures or tables, etc.</u> to support your responses. The figures and tables inserted in Markdown cells must be reproducable by your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 [4 marks]\n",
    "<u>Sensitivity</u> and <u>specificity</u> are two model evaluation metrics.  A good model should have both sensitivity and specificity high. Use the $2 \\times 2$ confusion matrix returned by `evaluate()` to calculate the sensitivity and specificity. Do you see a difference between them? If so, what causes this difference? Provide suggestions to improve the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.86\n",
      "f1-score:  0.9078947368421053\n",
      "confusion matric:  [[69, 8], [6, 17]]\n",
      "sensitivity:  0.8961038961038961\n",
      "specificity:  0.7391304347826086\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "accuracy,matrix,f1=evaluate(predicted_label,y_test)\n",
    "print(\"accuracy: \",accuracy)\n",
    "print(\"f1-score: \",f1)\n",
    "print(\"confusion matric: \",matrix)\n",
    "\n",
    "TP = matrix[0][0]\n",
    "FN = matrix[0][1]\n",
    "FP = matrix[1][0]\n",
    "TN = matrix[1][1]\n",
    "\n",
    "\n",
    "sensitivity = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP)\n",
    "print(\"sensitivity: \",sensitivity)\n",
    "print(\"specificity: \",specificity)\n",
    "x = []\n",
    "x.append(\"sensitivity\")\n",
    "x.append(\"specificity\")\n",
    "y=[]\n",
    "y.append(sensitivity)\n",
    "y.append(specificity)\n",
    "#use a bar plot to show the difference between sensitivity and specificity\n",
    "plt.title(\"Sensitivity vs Specificity\")\n",
    "plt.xlabel(\"evaluaton metrics\")\n",
    "plt.ylabel(\"evaluation\")\n",
    "plt.bar(x,y,width=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer of 150-200 words in this cell.\n",
    "\n",
    "The code and the histogram clearly show that sencitivity which value is 0.89614 is higher than specificity with value of 0.739134.Higher sencitivity means the model has a good and significant prediction effect for positive(\"<=50K\") instances, and a moderate prediction effect for negative(\">50K\") instances.In addition,\n",
    "sensitivity is TP/(TP+FN) and represents how many of the actual positive instances are found by the model and specificity is TN/(TN+FP) and represents how many of the actual negative instances are found by the model.There are two possible reasons for sensitivity higher than specificity. First, the data distribution is extremely uneven. In the process of processing data, it is not difficult to find that the number of instances with label \"<=50K\" is much more than the number of instances with label \">50K\". This causes the model to tend to predict an instance as positive(\"<=50K\"), which means that an instance is more likely to be predicted as positive(\"<=50K\"). The majority of instances with positive labels will result in a much higher priority probability than negative ones. And this is likely to make TP larger and TN smaller, and TP will be less affected by FN than TN by FP. Therefore sencitivity is higher than specificity. Second, since there is a missing value in the data itself, different processing methods for missing value and 0 probability will also affect the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 [4 marks]\n",
    "You can adopt different methods for training and/or testing, which will produce different results in model evaluation. \n",
    "\n",
    "(a) Instead of Gaussian, <u>implement KDE</u> for  $P(X_i|c_j)$ for numeric attributes $X_i$. Compare the evaluation results with Gaussian. Which one do you think is more suitable to model $P(X_i|c_j)$, Gaussian or KDE? Observe all numeric attributes and justify your answer.\n",
    "\n",
    "You can choose an arbitrary value for kernel bandwidth $\\sigma$ for KDE, but a value between 3 and 15 is recommended. You should write code to implement KDE, not call an existing function/method such as `KernelDensity` from `scikit-learn`.\n",
    "\n",
    "(b) Implement <u>10-fold and 2-fold cross-validations</u>.  \n",
    "\tObserve the evaluation results in each fold and the average accuracy, recall and specificity over all folds. \n",
    "\tComment on what is the effect by changing the values of $m$ in $m$-fold cross validation. (You can choose either Gaussian or KDE Naive Bayes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "KDE accuracy:  0.85\n",
      "KDE f1 score:  0.9006622516556292\n",
      "KDE confusion matrix:  [[68, 9], [6, 17]]\n",
      "0.8961038961038961\n",
      "0.7391304347826086\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "#Q2(a)\n",
    "def gaussian(x,xi,bandwidth):\n",
    "    return np.exp((-1/2)*(np.square((x-xi)/bandwidth)))/(bandwidth*np.sqrt(2*np.pi))\n",
    "#define the KDE pdf\n",
    "def KDE_pdf(x,c_attr,bandwidth):\n",
    "    KDE_lld = []\n",
    "    for i in range(len(c_attr)):\n",
    "        xi = c_attr[i]\n",
    "        KDE_lld.append(gaussian(x,xi,bandwidth))\n",
    "        \n",
    "    return np.log(np.mean(KDE_lld))\n",
    "\n",
    "#a function used to find the KDE likelihood of each numeric instance\n",
    "def get_KDE_likelihood(x_train,x_test,y_train,bandwidth):\n",
    "    \n",
    "    c1_attr = []\n",
    "    c2_attr = []\n",
    "    value1 = 0\n",
    "    value2 = 0\n",
    "    \n",
    "    c1_KDE_list = []\n",
    "    c2_KDE_list = []    \n",
    "    \n",
    "    for h in range(len(x_test)):\n",
    "        c1_lld = []\n",
    "        c2_lld = []\n",
    "        for i in range(len(num_index)):\n",
    "            c1_attr = []\n",
    "            c2_attr = []\n",
    "            for j in c1_index:\n",
    "                value1 = x_train[j][num_index[i]]#attribute value with label c1\n",
    "                c1_attr.append(int(value1))\n",
    "            for k in c2_index:\n",
    "                value2 = x_train[k][num_index[i]]#attribute value with label c2\n",
    "                c2_attr.append(int(value2))\n",
    "            \n",
    "            x = int(x_test[h][num_index[i]])#the numeric attribute values of x_test\n",
    "            #assign the values to function KDE_pdf to get the KDE likelihood of each label\n",
    "            c1_lld.append(KDE_pdf(x,c1_attr,bandwidth))\n",
    "            c2_lld.append(KDE_pdf(x,c2_attr,bandwidth))\n",
    "            \n",
    "        c1_KDE_list.append(c1_lld)\n",
    "        c2_KDE_list.append(c2_lld)\n",
    "    \n",
    "    return c1_KDE_list,c2_KDE_list\n",
    "\n",
    "\n",
    "def KDE_train(x_train,x_test,y_train,bandwidth):\n",
    "    \n",
    "    c1_index,c2_index = get_label_index(y_train)\n",
    "    \n",
    "    num_index,nom_index =get_num_and_nom_index(x_train)\n",
    "    \n",
    "    c1_list,c2_list = get_KDE_likelihood(x_train,x_test,y_train,bandwidth)\n",
    "    c1_lld_list,c2_lld_list,attr_list = get_likelihood(x_train,y_train,nom_index)\n",
    "    \n",
    "    return c1_list,c2_list,c1_lld_list,c2_lld_list\n",
    "\n",
    "\n",
    "#a function was created to find the sum KDE likelihood of each test numeric instance\n",
    "def sum_KDE_lld(x_test,x_train,y_train,bandwidth):\n",
    "    c1_KDE_list,c2_KDE_list = get_KDE_likelihood(x_train,x_test,y_train,bandwidth)\n",
    "    c1_KDE_lld = []\n",
    "    c2_KDE_lld = []\n",
    "    for i in range(len(c1_KDE_list)):\n",
    "        c1_lld = 0 \n",
    "        for j in range(len(c1_KDE_list[i])):\n",
    "            c1_lld += c1_KDE_list[i][j]\n",
    "        c1_KDE_lld.append(c1_lld)\n",
    "    for i in range(len(c2_KDE_list)):\n",
    "        c2_lld = 0 \n",
    "        for j in range(len(c2_KDE_list[i])):\n",
    "            c2_lld += c2_KDE_list[i][j]\n",
    "        c2_KDE_lld.append(c2_lld)\n",
    "        \n",
    "    return c1_KDE_lld,c2_KDE_lld\n",
    "\n",
    "    \n",
    "#a function used to predict the label of each test instance\n",
    "def KDE_predict(x_test,prior,bandwidth):\n",
    "    \n",
    "    predicted_label2 = []\n",
    "    c_hat2 = []\n",
    "    c1_prior = np.log(len(c1_index)/len(y_train))\n",
    "    c2_prior = np.log(len(c2_index)/len(y_train))\n",
    "    \n",
    "    c1_numeric_lld,c2_numeric_lld = sum_KDE_lld(x_test,x_train,y_train,bandwidth)\n",
    "    c1_nominal_lld,c2_nominal_lld = sum_likelihood(x_test,c1_lld_list,c2_lld_list)\n",
    "    \n",
    "    \n",
    "    list3 = []\n",
    "    list4 = []\n",
    "    for i in range(len(x_test)):\n",
    "        c1_posterior = c1_prior + c1_numeric_lld[i] + c1_nominal_lld[i]\n",
    "        c2_posterior = c2_prior + c2_numeric_lld[i] + c2_nominal_lld[i]\n",
    "        list3.append(c1_posterior)\n",
    "        list4.append(c2_posterior)\n",
    "         \n",
    "    for j in range(len(x_test)):\n",
    "        \n",
    "        c_hat2.append(max(list3[j],list4[j]))\n",
    "        \n",
    "        if (list3[j] < list4[j]):\n",
    "            predicted_label2.append(\" >50K\")\n",
    "        else:\n",
    "            predicted_label2.append(\" <=50K\")\n",
    "            \n",
    "    return predicted_label2,c_hat2\n",
    "\n",
    "#a function used to evaluate the method of prediction\n",
    "def KDE_evaluate(x_train,x_test,y_train,y_test,bandwidth):\n",
    "    c1_list,c2_list,c1_lld_list,c2_lld_list = KDE_train(x_train,x_test,y_train,bandwidth)\n",
    "    predicted_label2,c_hat2 =  KDE_predict(x_test,prior,bandwidth)\n",
    "    \n",
    "    return evaluate(predicted_label2,y_test)\n",
    "\n",
    "\n",
    "predicted_label2,c_hat2 = KDE_predict(x_test,prior,3)\n",
    "accuracy_kde,matrix_kde,f1_kde = evaluate(predicted_label2,y_test)\n",
    "print(evaluate(predicted_label2,y_test)[0])\n",
    "print(\"KDE accuracy: \",accuracy_kde)\n",
    "print(\"KDE f1 score: \",f1_kde)\n",
    "print(\"KDE confusion matrix: \",matrix_kde)\n",
    "\n",
    "TP = matrix[0][0]\n",
    "FN = matrix[0][1]\n",
    "FP = matrix[1][0]\n",
    "TN = matrix[1][1]\n",
    "sensitivity = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP)\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "\n",
    "KDE_accur_list = []\n",
    "GN_accur_list = []\n",
    "bandwidth_list = []\n",
    "for i in range(3,16):\n",
    "    bandwidth_list.append(i)\n",
    "    KDE_accur_list.append(KDE_evaluate(x_train,x_test,y_train,y_test,i)[0])\n",
    "    GN_accur_list.append(evaluate(predicted_label,y_test)[0])\n",
    "       \n",
    "plt.plot(bandwidth_list,GN_accur_list)\n",
    "plt.plot(bandwidth_list,KDE_accur_list)\n",
    "plt.ylim(0.75,0.90)\n",
    "plt.title(\"Comparison in Performance of KDE naive Bayes and Gaussian naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy list of 10-fold Cross Validation: [0.83, 0.83, 0.8, 0.86, 0.88, 0.82, 0.82, 0.76, 0.83, 0.86]\n",
      " \n",
      "Recall list of 10-fold Cross Validation: [0.8133333333333334, 0.8205128205128205, 0.8311688311688312, 0.8701298701298701, 0.925, 0.8354430379746836, 0.8701298701298701, 0.8289473684210527, 0.8767123287671232, 0.8961038961038961]\n",
      " \n",
      "Specificity list of 10-fold Cross Validation: [0.88, 0.8636363636363636, 0.6956521739130435, 0.8260869565217391, 0.7, 0.7619047619047619, 0.6521739130434783, 0.5416666666666666, 0.7037037037037037, 0.7391304347826086]\n",
      " \n",
      "Accuracy list of 2-fold Cross Validation: [0.84, 0.818]\n",
      "Recall list of 2-fold Cross Validation: [0.8527131782945736, 0.8612565445026178]\n",
      "Specificity list of 2-fold Cross Validation: [0.7964601769911505, 0.6779661016949152]\n",
      "For 10-fold CV: the average accuracy: 0.8290000000000001 the average recall: 0.8567481356541482 the average specificity: 0.7363954974172364\n",
      " \n",
      "For 2-fold CV: the average accuracy: 0.829 the average recall: 0.8569848613985958 the average specificity: 0.7372131393430328\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAho0lEQVR4nO3deZwU1fnv8c8TwEzYFRGRASGEyCYiIqhBEXFBMUFEDYNGcONiJDEaEzG5MZrkGr1uJFcM4eeGmAhGBZcQDFFZBBFBEUFAEYmMoigogkoi+tw/6ozW9HTPNDA1W33fr1e/purUqaqn+/T006dOdZW5OyIikl5fq+4ARESkeikRiIiknBKBiEjKKRGIiKScEoGISMopEYiIpJwSgdQYZnaxmb1rZtvNrEUFdeeY2YU5lrU3Mzez+slEWjOY2XfM7LXwep1WQd1rzOy+cpavN7PjKz1IqRWUCGS3mdlYM1tiZv8xs3uyLB9oZqvN7BMze9rMDixnWw2AW4AT3b2xu29OMHTMbESIfbuZbTSzf5hZPzMrCh+KllG/vpltMrNTE4pnPzO738zeNrOtZrbAzPpWsNpvgNvC6zUjibhi8fUxs5lm9qGZbTGzxWZ2npm1MbOdZtYxyzrTzeymJOOSyqFEIHvibeB3wF2ZC8xsX+Bh4FfAPsASYFo522oFFAArKz/MMrFdDowHrgv7bQfcDgwBpgPNgf4Zqw0CHJiVUFiNgeeBw4her8nA382scTnrHEjVvF5HAk8Bc4FvAS2Ai4GT3f0t4EngBxnr7AOcQvQ8pKZzdz302KMHUTK4J6NsNLAwNt8I+BTonGX9bwMfE33QbgeeCuVHEX04bg1/j4qtMwe4MEzXA24C3gfWAZeEbdXPsq9mYR9nlvN8JgF3ZZQ9ANySpe7XgQ+B7rGyluG57gfsCzwe6mwB5gNfy/N1/Qg4LMey14Evwn62hzgOAB4N+1kLXBSrfw1wX2z+B8C/gc3AL4H1wPE59vUMMKGcOEcAr2eU/RB4obrfm3rk91CPQJLSDXipZMbdPyb68OqWWdHdX42VN3f348I3yr8DfyT6BnoL0TfkbGMHFwGnAocCvYEzyonrSKKex/Ry6kwGzjCzbwCYWTPgu8C9WWL/D1HPpyhWfBYw1903AT8FiomSQyvgF0RJqlxm1hPYi+gDvQx37wi8CXzXo0ND/wHuD/s6gOg1uM7MBmbZdlfgT0TJ4ACi17cwRxwNiV6zB8sJdzqwr5n1i5X9gCyvl9RMSgSSlMZE3+TjtgJN8lx/MPCau09x953ufj+wmugDOdNZwHh33+DuW4Dfl7PdFsD77r4zVwV3XwC8CwyNbf9Vd1+WY5W/UjoRjAhlAJ8BrYED3f0zd5/v4StzLmbWFJgCXOvuma9hrnXaAv2AK919R4j1DjIO2QRnAI+7+7yQQH5F1LvIZm+iz4mNufbt7p8CfwPODbF0IjrE9ddc60jNokQgSdkONM0oawpsM7N2YZB2u5ltz7H+AUSHLuL+DbTJUXdDRr1cNhN9e63ojKJ7CR9sRB+m5R3rfgr4hpn1DQPiPfmqx3Ej0bf6f5rZOjMbV95OQy/kMWCRu/8+Vr4y9podnWXVA4At7r4tVpbX6xV6a7kG5z8gShKty4ub6PU5y8wKiF6vWaFHJLWAEoEkZSVwSMmMmTUCOgIr3f3NcDijsbvnGgx9m2gwNK4d8FaWuhuBthn1cnkW2AGcVn743AsMDAOlR1DOt1t3/4JoDKGIqDfweMkHsrtvc/efuvs3iXozl2c7XANgZl8HZhA9x/+VsY9usddsfpbV3wb2MbN4jyuv1ysc/sl6uq67f0L0mg3LtjxWbz5RMhkCnIMOC9UqSgSy28IplQVEg7X1zKwg9k17OtDdzIaFOlcDy919dZ6bnwl8O5zmWd/Mvg90JRp4zfQA8GMzKzSzvYGc37rDoZargQlmdpqZNTSzBmZ2spn931i9fxMNkt4PzHb3dyqI96/A94GziSUNMzvVzL4VTkf9CPg8PEoJp88+SDT4e25ILnlz9w3AQuD3oR16ABcAf8lS/UHg1HC67F5Ep6GW91nwc2CUmf2sZIzGzA4xs6kZ9e4FbiA66+qxXYlfqll1j1brUXsfRGeieMbjmtjy44mO639KdJZP+3K21Z6MM32IjnkvJRpbWAr0iy2bw1dnDdUHbiX6RvoG5Zw1FFv/bKJTWj8G3iEamD4qo86osJ3v5/l6rCU6Y2evWNllRGfkfEw0kPurHOv2D/v6hOiwWsnj6HL2t57YmT5EA76PhxheB8ZktFX8rKGRRIPNFZ41FOr3Af4R2mIL8BxRworX6UB0GOlP1f3e1GPXHhYaUEREUkqHhkREUi6xRGBmd4Wf5K/IsdzM7I9mttbMlptZr6RiERGR3JLsEdxD9LP8XE4GOoXHaKIfuIiISBVLLBG4+zyiQaVchgD3emQR0NzMKjpXWUREKll1Xqa3DaV/BFQcysr8gtHMRhP1GmjUqNFhnTt3rpIARUTqiqVLl77v7i2zLavORGBZyrKewuTuk4guBEbv3r19yZIlScYlIlLnmFnOX9xX51lDxZT+NWgh0a8jRUSkClVnIngUODecPXQEsNXdc17YSkREkpHYoSEzux84lugCX8XAr4EGAO4+kegSAqcQ/RrzE+C8pGIREZHcEksE7l5UwXInuhSAiKTIZ599RnFxMTt27KjuUOqkgoICCgsLadCgQd7r1Ombe4tIzVNcXEyTJk1o3749GbeGlj3k7mzevJni4mI6dOiQ93q6xISIVKkdO3bQokULJYEEmBktWrTY5d6WEoGIVDklgeTszmurRCAiknIaIxCRatV+3N8rdXvrrx9cqdtLA/UIRCRVzj//fPbbbz+6d+9eqnzLli2ccMIJdOrUiRNOOIEPPvig3O0UFRXRo0cPbr311px1Ro0axYMPPlimfM6cOZx66qlZ11m8eDHHHHMMBx10EJ07d+bCCy/klVdeobCwkC++KH3jup49e7J48eJy48yHEoGIpMqoUaOYNWtWmfLrr7+egQMH8tprrzFw4ECuv/76nNt45513WLhwIcuXL+eyyy6rtNjeffddzjzzTG644QbWrFnDqlWrGDRoEC1atKBt27bMn//V7apXr17Ntm3b6NOnzx7vV4lARFLlmGOOYZ999ilT/sgjjzBy5EgARo4cyYwZM3Ju48QTT2TTpk307NmT+fPns2zZMo444gh69OjB0KFDs/YmZs2aRefOnenXrx8PP/xw1u1OmDCBkSNHcuSRRwLRwO8ZZ5xBq1atKCoqYurUr24TPXXqVIqKyv25Vt6UCEREiL6Nt24dXQm/devWbNq0KWfdRx99lI4dO7Js2TKOPvpozj33XG644QaWL1/OwQcfzLXXXluq/o4dO7jooot47LHHmD9/Pu+8807W7a5YsYLDDjss67KzzjqLGTNmsHPnTgCmTZvG8OHDd+eplqFEICKyB7Zu3cqHH35I//79gag3MW/evFJ1Vq9eTYcOHejUqRNmxjnnnLPL+9l///3p1q0bTz75JMuWLaNBgwZlxjl2lxKBiAjQqlUrNm6Mrnu5ceNG9ttvv0rdfj7n93fr1o2lS5fmXF5yeKgyDwuBTh8VkWpWU073/N73vsfkyZMZN24ckydPZsiQIXmt16xZM/bee2/mz5/P0UcfzZQpU77sHZTo3Lkzb7zxBq+//jodO3bk/vvvz7qtsWPH0qdPHwYPHkzfvn0BuO+++zj++OPZf//9GTZsGL/4xS9o2LAhTz311J494Rj1CEQkVYqKijjyyCNZs2YNhYWF3HnnnQCMGzeO2bNn06lTJ2bPns24cePy3ubkyZP52c9+Ro8ePVi2bBlXX311qeUFBQVMmjSJwYMH069fPw488MCs22nVqhVTp07liiuu4KCDDqJLly7Mnz+fpk2bAtC8eXOOOOIIWrVqtUvXEqqIRRcBrT10hzKR2m3VqlV06dKlusOo07K9xma21N17Z6uvHoGISMppjEBEJIcnnniCK6+8slRZhw4dmD59ejVFlAwlAhGRHE466SROOumk6g4jcTo0JCKSckoEIiIpp0QgIpJyGiMQkep1TbNK3t7Wyt1eCqhHICKpsWHDBgYMGECXLl3o1q0bf/jDH8qtv3r1anr27Mmhhx7K66+/nrNe48aNs5bnuh8BwE033UTnzp3p3r07hxxyCPfeey/XXHMNV111Val6y5YtS/x3F0oEIpIa9evX5+abb2bVqlUsWrSICRMm8Morr+SsP2PGDIYMGcKLL75Ix44dKy2OiRMnMnv2bBYvXsyKFSuYN28e7k5RURHTpk0rVXfq1KmMGDGi0vadjRKBiKRG69at6dWrFwBNmjShS5cuvPXWW1nrzpw5k/Hjx3PHHXcwYMAAAG655Ra6d+9O9+7dGT9+fJl13J2xY8fStWtXBg8enPNS1tdddx233377l5eOaNasGSNHjuSggw6iefPmPPfcc1/WfeCBByrtctO5aIxARFJp/fr1vPjii19e3C3TKaecwpgxY2jcuDFXXHEFS5cu5e677+a5557D3enbty/9+/fn0EMP/XKd6dOns2bNGl5++WXeffddunbtyvnnn19qu9u2bWPbtm05exglVxjt27cvixYtokWLFnTq1KnynngWSgQilamyBz73hAZNc9q+fTvDhg1j/PjxX34rr8gzzzzD0KFDadSoEQCnn3468+fPL5UI5s2bR1FREfXq1eOAAw7guOOOK7Mddy/3ktTDhw/nqKOO4uabb670y03nokQgtVr7cX+v7hBKWV9Q3RFIRT777DOGDRvG2Wefzemnn573evleoLOi+w40bdqURo0asW7dOr75zW+WWd62bVvat2/P3Llzeeihh3j22WfzjnF3KRGISPWqwp6Lu3PBBRfQpUsXLr/88l1a95hjjmHUqFGMGzcOd2f69OlMmTKlTJ0///nPnHvuuWzatImnn34660DvVVddxSWXXMK0adNo2rQpH330EVOnTmX06NFAdHjosssuo2PHjhQWFu7+E86TBotFJDUWLFjAlClTeOqpp+jZsyc9e/Zk5syZea3bq1cvRo0aRZ8+fejbty8XXnhhqcNCAEOHDqVTp04cfPDBXHzxxWVuUFPi4osvZsCAARx++OF0796d/v3707Bhwy+Xn3nmmaxcuTLxQeISuh+B1Go179BQsqf57ZIaOkag+xEkT/cjEBGRXaIxAhFJvUsuuYQFCxaUKrv00ks577zzqimiqpWqRKDDCOWooYcRpG6q6BTKqjZhwoTqDqHS7M7hfh0aEpEqVVBQwObNm3frA0vK5+5s3ryZgoJdO485VT0CEal+hYWFFBcX895771V3KHVSQUHBLp9ymmgiMLNBwB+AesAd7n59xvJmwH1AuxDLTe5+d5IxiUj1atCgAR06dKjuMCQmsUNDZlYPmACcDHQFisysa0a1S4BX3P0Q4FjgZjPbK6mYRESkrCTHCPoAa919nbv/F5gKDMmo40ATi0aNGgNbgJ0JxiQiIhmSTARtgA2x+eJQFncb0AV4G3gZuNTdv8jckJmNNrMlZrZExxVFRCpXkokg27lhmacJnAQsAw4AegK3mVmZSwG6+yR37+3uvVu2bFnZcYqIpFqSiaAYaBubLyT65h93HvCwR9YCbwCdE4xJREQyJJkIngc6mVmHMAA8HHg0o86bwEAAM2sFHASsSzAmERHJkNjpo+6+08zGAk8QnT56l7uvNLMxYflE4LfAPWb2MtGhpCvd/f2kYhIRkbIS/R2Bu88EZmaUTYxNvw2cmGQMIiJSPl1iQkQk5ZQIRERSTolARCTllAhERFJOiUBEJOWUCEREUk6JQEQk5ZQIRERSTncoE5EapybdXzwN9xZXj0BEJOWUCEREUk6JQEQk5ZQIRERSTolARCTllAhERFJOiUBEJOWUCEREUk6JQEQk5ZQIRERSTolARCTllAhERFJOiUBEJOWUCEREUk6JQEQk5ZQIRERSTolARCTllAhERFJOiUBEJOWUCEREUk6JQEQk5ZQIRERSTolARCTllAhERFJOiUBEJOUSTQRmNsjM1pjZWjMbl6POsWa2zMxWmtncJOMREZGy6ie1YTOrB0wATgCKgefN7FF3fyVWpzlwOzDI3d80s/2SikdERLLLq0dgZt8xs9lm9qqZrTOzN8xsXQWr9QHWuvs6d/8vMBUYklFnBPCwu78J4O6bdvUJiIjInsm3R3AncBmwFPg8z3XaABti88VA34w63wYamNkcoAnwB3e/N3NDZjYaGA3Qrl27PHcvIiL5yDcRbHX3f+ziti1LmWfZ/2HAQOAbwLNmtsjdXy21kvskYBJA7969M7chIiJ7IN9E8LSZ3Qg8DPynpNDdXyhnnWKgbWy+EHg7S5333f1j4GMzmwccAryKiIhUiXwTQckhnd6xMgeOK2ed54FOZtYBeAsYTjQmEPcIcJuZ1Qf2Cvu5Nc+YRESkEuSVCNx9wK5u2N13mtlY4AmgHnCXu680szFh+UR3X2Vms4DlwBfAHe6+Ylf3JSIiuy+vRGBmzYBfA8eEornAb9x9a3nruftMYGZG2cSM+RuBG/MNWEREKle+Pyi7C9gGnBUeHwF3JxWUiIhUnXzHCDq6+7DY/LVmtiyBeEREpIrl2yP41Mz6lcyY2XeAT5MJSUREqlK+PYKLgclhrMCALcCopIISEZGqk+9ZQ8uAQ8ysaZj/KMmgRESk6pSbCMzsHHe/z8wuzygHwN1vSTA2ERGpAhX1CBqFv02yLNOlHkRE6oByE4G7/zlM/svdF8SXhQFjERGp5fI9a+j/5VkmIiK1TEVjBEcCRwEtM8YJmhJdNkJERGq5isYI9gIah3rxcYKPgDOSCkpERKpORWMEc4G5ZnaPu/+7imISEZEqlO8Pyj4J9yPoBhSUFLp7eZehFhGRWiDfweK/AKuBDsC1wHqi+w2IiEgtl28iaOHudwKfuftcdz8fOCLBuEREpIrke2jos/B3o5kNJrrlZGEyIYmISFXKNxH8Llxw7qdEvx9oClyWWFQiIlJl8r3o3ONhciuwy7etFBGRmivfW1XeTZZrC4WxAhERqcXyPTT0eGy6ABhKNE4gIiK1XL6Hhh6Kz5vZ/cC/EolIRESqVL6nj2bqBLSrzEBERKR65DtGsI1ojMDC33eAKxOMS0REqki+h4ay3ZhGRETqgIouQ92rvOXu/kLlhiMiIlWtoh7BzeUsc0AXnRMRqeUqugy1fjwmIlLH5fs7AsysO9CV0pehvjeJoEREpOrke9bQr4FjiRLBTOBk4BlAiUBEpJbL93cEZwADgXfc/TzgEODriUUlIiJVJt9E8Km7fwHsNLOmwCbgm8mFJSIiVSXfMYIlZtYc+B9gKbAdWJxUUCIiUnXy/UHZD8PkRDObBTR19+XJhSUiIlUlr0NDZvaImY0ws0buvl5JQESk7sh3jOAWoB/wipn9zczOMLOCilYSEZGaL69EEG5Y/0OiAeJJwFlEA8blMrNBZrbGzNaa2bhy6h1uZp+b2Rn5Bi4iIpUj78tQm9k3gGHAGOBwYHIF9esBE4h+c9AVKDKzrjnq3QA8kX/YIiJSWfIdI5gGrCK6ttAEoKO7/6iC1foAa919nbv/F5gKDMlS70fAQ+TRwxARkcqX7+mjdwMj3P3zXdh2G2BDbL4Y6BuvYGZtiG57eRxRLyMrMxsNjAZo1073wxERqUzl9gjM7OcA7j4LOD1j2XUVbNuylHnG/HjgyooSjLtPcvfe7t67ZcuWFexWRER2RUWHhobHpq/KWDaognWLgbax+ULK3vC+NzDVzNYTXcbidjM7rYLtiohIJaro0JDlmM42n+l5oJOZdQDeIkoqI+IV3L3Dlxszuwd43N1nVLBdERGpRBUlAs8xnW2+9EL3nWY2luhsoHrAXe6+0szGhOUTdzVYERGpfBUlgkPM7COib//fCNOE+Qp/UObuM4kuWx0vy5oA3H1UhdGKiEilq+gOZfWqKhAREakeef+gTERE6iYlAhGRlFMiEBFJOSUCEZGUUyIQEUk5JQIRkZRTIhARSTklAhGRlFMiEBFJOSUCEZGUUyIQEUk5JQIRkZRTIhARSTklAhGRlFMiEBFJOSUCEZGUUyIQEUk5JQIRkZRTIhARSTklAhGRlFMiEBFJOSUCEZGUUyIQEUk5JQIRkZRTIhARSTklAhGRlFMiEBFJOSUCEZGUUyIQEUk5JQIRkZRTIhARSTklAhGRlFMiEBFJuUQTgZkNMrM1ZrbWzMZlWX62mS0Pj4VmdkiS8YiISFmJJQIzqwdMAE4GugJFZtY1o9obQH937wH8FpiUVDwiIpJdkj2CPsBad1/n7v8FpgJD4hXcfaG7fxBmFwGFCcYjIiJZJJkI2gAbYvPFoSyXC4B/ZFtgZqPNbImZLXnvvfcqMUQREUkyEViWMs9a0WwAUSK4Mttyd5/k7r3dvXfLli0rMUQREamf4LaLgbax+ULg7cxKZtYDuAM42d03JxiPiIhkkWSP4Hmgk5l1MLO9gOHAo/EKZtYOeBj4gbu/mmAsIiKSQ2I9AnffaWZjgSeAesBd7r7SzMaE5ROBq4EWwO1mBrDT3XsnFZOIiJSV5KEh3H0mMDOjbGJs+kLgwiRjEBGR8umXxSIiKadEICKSckoEIiIpp0QgIpJySgQiIimnRCAiknJKBCIiKadEICKSckoEIiIpp0QgIpJySgQiIimnRCAiknJKBCIiKadEICKSckoEIiIpp0QgIpJySgQiIimnRCAiknJKBCIiKadEICKSckoEIiIpp0QgIpJySgQiIimnRCAiknJKBCIiKadEICKSckoEIiIpp0QgIpJySgQiIimnRCAiknJKBCIiKadEICKSckoEIiIpp0QgIpJySgQiIimXaCIws0FmtsbM1prZuCzLzcz+GJYvN7NeScYjIiJlJZYIzKweMAE4GegKFJlZ14xqJwOdwmM08Kek4hERkeyS7BH0Ada6+zp3/y8wFRiSUWcIcK9HFgHNzax1gjGJiEiG+gluuw2wITZfDPTNo04bYGO8kpmNJuoxAGw3szWVG2r1MNgXeL+64wDgWqvuCOoEtWndU4fa9MBcC5JMBNki9t2og7tPAiZVRlA1iZktcffe1R2HVB61ad2ThjZN8tBQMdA2Nl8IvL0bdUREJEFJJoLngU5m1sHM9gKGA49m1HkUODecPXQEsNXdN2ZuSEREkpPYoSF332lmY4EngHrAXe6+0szGhOUTgZnAKcBa4BPgvKTiqaHq3OEuUZvWQXW+Tc29zCF5ERFJEf2yWEQk5ZQIRERSrs4lAjMbamZuZp2rO5Z8mNmhId6TqjuW2qo2tbmZrTezl8MlVeaaWc5zu3dz+3PMrHdsX/tW5vb3RC1rp/Nj7bTCzDJ/DLun27+j5EoLZnamma0ys6fNrLeZ/bGCdWeaWfPw+GFlxFPnEgFQBDxDdJbSHguXykhSSbxFSe6kCp5HdaptbT7A3XsAc4D/nfC+apJa0U5mVgj8EugX2ukIYHll7sPdL3T3V8LsBcAP3X2Auy9x9x9XsO4p7v4h0ByolESAu9eZB9AYeAv4NrA6lJ0MPBCrcyzwWJg+EXgWeAH4G9A4lK8HruarN+1FRKfDvgQ8BDQM9ToCi8Ky3wDbY/v5WShfDlybI14D1oXtvA0UxJb9HHg57PP6UPYt4F+h7IWw3rHA47H1bgNG7eLzaAVMD+UvAUcBvwUujW33/wA/ru42rgNtvh7YN0wPAmaG6ZZhP8+Hx3diz+/u8F5YDgwL5X8ClgAr4/siSi69M/dV3Y/a1E5AL2AZUC/LsjnAeGAhsALoE8obAXeF7b4IDAnl9YCbYu33o3g7heeyHVgD3Ejs/7mctl9P9GvnqcCnIdYbgSkl+w31/gJ8L6/2qe43SCW/2c4B7gzTC0OD1gfeBBrF/oHOCS/kvFj5lcDVsRf657HttohN/y7WmI8DRWF6TMmbLbyJJxF90H8t1DsmS7z9gCfD9F+B02P/IAtjb+p9wt/ngKFhugBoSMWJIJ/nMQ34SeyN2wxoD7wQyr4GvB5fv6Y8amGbr+erRDAeGB1r/35huh2wKkzfAIyPrb93xnuiHtGHSo8wP4eamQhqTTuF1/SJENvdwHdjy+YA/xOmjwFWhOnrgHPCdHPgVaLkcDFRgqqf0W7xdopPH8tXiSBX268Pr1H7kv2H8v7AjDDdDHijZL8VPeraoaEioixJ+Fvk7juBWcB3zaw+MBh4hKi71xVYYGbLgJGUvhbHtNh0dzObb2YvA2cD3UL5kUTfViD6Ry5xYni8SPSNpjPRFVYrjDdMHw/c7e6fALj7FjNrArRx9+mhbEfJ8grk8zyOI1z51d0/d/et7r4e2Gxmh5Y8F3ffnMf+qlpta3OAp81sE1E7l2zjeOC2ENejQNPQ5scTXcUXAHf/IEyeZWYvhP11C8+rJqs17eTunxP11s4g+kC/1cyuiVW5P9SbR9ROzcM2x4V45xB9UWtH1H4Tw3PF3beU+yqVlqvts3L3ucC3zGw/otf7oZL9ViTJaw1VKTNrQfSB1t3MnCiru5n9nOiNcwmwBXje3beZmQGz3T3XsfmPY9P3AKe5+0tmNoooa5cbDvB7d/9zOfHWA4YB3zOzX4Z1WoR/fiO/6zIB7KT0WE9BxvI9eR53AKOA/Ym6vTVKbWvzmAFhX/cQHba4nKgNj3T3TzOeY5n3gpl1AK4ADnf3D8zsHsq2e41RG9vJo6/Vi4HFZjabqGdwTcnizOphu8PcvdQFMbO13y7YnXWnECXE4cD5+a5Ul3oEZxBd0vpAd2/v7m2Jukb9iDJ0L6LjiSXfJhYB3zGzbwGYWUMz+3aObTcBNppZA6IXucQiog9zKD0A9gRwvpk1DttuE7J03PHAS+7eNsR7IFEX8jTgn2H9hmH9fdz9I6DYzE4LZV8Py/8NdA3zzYCB5bxGuZ7Hk0RdWMysnpk1DeXTib4ZHR6eU01T29r8S+ED/ydEl1jZh6jNx5YsN7OeYTKzfG+gKdGH4VYza0V0KLEmq1XtZGYHWOmbZPUk+j8r8f1Qrx/RZXG2hu3+KHzwE3rSELXfmNDjIbR1vrK1fdw2oucfdw/R+wp3X5nvjupSIigi+uCKewgYEbp6jxP9wzwO4O7vEX3bvd/MlhO9cXKd1vYrouPzs4HVsfKfAJeb2WKgNbA1bPufRN3RZ0OX9UHKNlh58c4iOjywJHQ1rwjLfwD8OMS7ENjf3TcADxANJv2FqMubS67ncSkwIMS6lNC99ug+Ek8TDeh9Xs52q0tta/NSPLqu1v1E34h/DPS26HTFV4iOa0N03Htvi05hfInojKOXiNp5JVFPbUF5+6kBals7NQBuMrPV4f/v+0T/IyU+MLOFwESiM34gOrmiAbDczFaEeYh61W+G8peAETmeRzZl2j6+MByqXRCW3xjK3gVWEfVg8qZLTOyB8I38U3d3MxtOdNxzSHXHVVnM7GtEx1HPdPfXqjuemqCut3ldkVQ7mdkc4Ap3X7Kn20pCeN4vA71CTyUvdWaMoJocRjTAZ8CH7MIxuZrOoh+7PA5MVxIopc62eR2TunYys+OJeoi37EoSAPUIRERSry6NEYiIyG5QIhARSTklAhGRlFMiEBFJOSUCEZGU+/9uOKFHeRkO7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Q2(b)\n",
    "#define a function to find the index of train data\n",
    "def find_train_index(index_list,train_list):\n",
    "    train_index = []\n",
    "    for i in index_list:\n",
    "        for j in range(len(train_list)):\n",
    "            if (i == j):\n",
    "                train_list.remove(j)\n",
    "                \n",
    "    train_index = train_list\n",
    "    return train_index\n",
    "\n",
    "#a function helps splitting the data \n",
    "def get_data_list(index_list,a_list):\n",
    "    data_list = []\n",
    "    for i in index_list:\n",
    "        data_list.append(a_list[i])\n",
    "    return data_list\n",
    "\n",
    "#a function used to split the full dataset and use the method of cross validation\n",
    "def k_fold_cv(k,filename):\n",
    "\n",
    "    attribute_list = []\n",
    "    label_list = []\n",
    "    with open(filename,\"r\") as f:#read and open the file \n",
    "        for i in f.readlines()[1:]:\n",
    "            attribute_list.append(i.strip().split(\",\")[:-1])\n",
    "            label_list.append(i.strip().split(\",\")[-1]) \n",
    "        \n",
    "    f.close()\n",
    "    #calculate the length of the data before splitting\n",
    "    f_len = len(attr)\n",
    "    #define the split ration\n",
    "    ratio = 1/k\n",
    "    n = int(ratio*f_len)\n",
    "    accur_list = []\n",
    "    recall_list = []\n",
    "    specificity_list = []\n",
    "    \n",
    "    train_index = []\n",
    "    #use a for loop to iterate the value of k\n",
    "    for i in range(k):\n",
    "        #find the index of train and test\n",
    "        train_list = list(range(f_len))\n",
    "        test_index = list(range(n*i,n*(i+1)))\n",
    "        train_index = find_train_index(test_index,train_list)\n",
    "        \n",
    "        x_test = get_data_list(test_index,attribute_list)\n",
    "        y_test = get_data_list(test_index,label_list)\n",
    "        x_train = get_data_list(train_index,attribute_list)\n",
    "        y_train = get_data_list(train_index,label_list)\n",
    "        \n",
    "        prior,c1_list,c2_list,c1_lld_list,c2_lld_list = train(x_train,y_train)\n",
    "        predicted_label3,c_hat3,list5,list6= predict(x_test,y_train)\n",
    "        accuracy,recall,specificity = cv_evaluate(predicted_label3,y_test)\n",
    "        accur_list.append(accuracy)\n",
    "        recall_list.append(recall)\n",
    "        specificity_list.append(specificity)\n",
    "    \n",
    "    return accur_list,recall_list,specificity_list\n",
    "\n",
    "#a function used to evaluate cross validation\n",
    "def cv_evaluate(predicted_label,y_test):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if (predicted_label[i] == \" <=50K\" and y_test[i] == \" <=50K\"):\n",
    "            TP += 1\n",
    "        elif (predicted_label[i] == \" >50K\" and y_test[i] == \" >50K\"):\n",
    "            TN += 1\n",
    "        elif (predicted_label[i]== \" <=50K\" and y_test[i] == \" >50K\"):\n",
    "            FP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "    #find the accuracy,recall and specificity\n",
    "    accuracy = (TP+TN)/(len(y_test))\n",
    "    recall = TP/(TP+FN)\n",
    "    specificity = TN/(TN+FP)\n",
    "    \n",
    "    return accuracy,recall,specificity\n",
    "        \n",
    "\n",
    "accur_list_10,recall_list_10,specificity_list_10 =k_fold_cv(10,\"adult.csv\")\n",
    "accur_list_2,recall_list_2,specificity_list_2 = k_fold_cv(2,\"adult.csv\")\n",
    "print(\"Accuracy list of 10-fold Cross Validation:\",k_fold_cv(10,\"adult.csv\")[0])\n",
    "print(\" \")\n",
    "print(\"Recall list of 10-fold Cross Validation:\",k_fold_cv(10,\"adult.csv\")[1])\n",
    "print(\" \")\n",
    "print(\"Specificity list of 10-fold Cross Validation:\",k_fold_cv(10,\"adult.csv\")[2])\n",
    "print(\" \")\n",
    "print(\"Accuracy list of 2-fold Cross Validation:\",accur_list_2)\n",
    "print(\"Recall list of 2-fold Cross Validation:\",recall_list_2)\n",
    "print(\"Specificity list of 2-fold Cross Validation:\",specificity_list_2)\n",
    "\n",
    "avg_10_fold_accur = np.mean(accur_list_10)\n",
    "avg_2_fold_accur = np.mean(accur_list_2)\n",
    "avg_10_fold_recall = np.mean(recall_list_10)\n",
    "avg_2_fold_recall = np.mean(recall_list_2)\n",
    "avg_10_fold_spec = np.mean(specificity_list_10)\n",
    "avg_2_fold_spec = np.mean(specificity_list_2)\n",
    "\n",
    "print(\"For 10-fold CV:\",\"the average accuracy:\",avg_10_fold_accur,\"the average recall:\",avg_10_fold_recall,\"the average specificity:\",avg_10_fold_spec)\n",
    "print(\" \")\n",
    "print(\"For 2-fold CV:\",\"the average accuracy:\",avg_2_fold_accur,\"the average recall:\",avg_2_fold_recall,\"the average specificity:\",avg_2_fold_spec)\n",
    "\n",
    "aters = (\"Average Accuracy\",\"Average Recall\",\"Average Specificity\")\n",
    "list_10 = [avg_10_fold_accur,avg_10_fold_recall,avg_10_fold_spec]\n",
    "list_2 = [avg_2_fold_accur,avg_2_fold_recall,avg_2_fold_spec]\n",
    "bar_width = 0.3\n",
    "index_10 = np.arange(len(aters))\n",
    "index_2 = index_10 + bar_width\n",
    "\n",
    "plt.bar(index_10,height=list_10,width=bar_width,label = \"10_fold CV\")\n",
    "plt.bar(index_2,height=list_2,width=bar_width,label=\"2_fold CV\")\n",
    "plt.legend()\n",
    "plt.xticks(index_10 + bar_width/2,aters)\n",
    "plt.ylabel(\"Evaluation\")\n",
    "plt.ylim(0,1)\n",
    "plt.title(\"10-fold CV vs 2-fold CV\")\n",
    "plt.show()\n",
    "\n",
    "plt.bar(list(range(10)),accur_list_10)\n",
    "plt.title(\"accuracies of 10-fold cross validation\")\n",
    "plt.xlabel(\"numbers of iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Provide your text answer of 150-200 words in this cell.\n",
    "\n",
    "\n",
    "Q2(a):\n",
    "The graph in Q2(a) compares the performance of KDE naive Bayes and Gaussian naive Bayes.It clearly shows that KDE naive Bayes is significantly affected by the value of bandwidth and Gaussian naive Bayes keeps constant.This means choose a different value of bandwidth results in a different shape of the resulting pdf.In addition,from the graph we can find that the accuracy of kde is generally lower than that of Gaussian. When the bandwidth is in the range of 1 to 3, the accuracy of kde is similar to that of gaussian, and the accuracy of kde decreases as the bandwidth goes on.In my opinion,I think Gaussian naive Bayes is more suitable to model likelihood because the parameter bandwidth of KDE is full of too much randomness and instability. On the contrary, Gaussian is not affected by parameters and the accuracy is stable and high.\n",
    "\n",
    "\n",
    "\n",
    "Q2(b):\n",
    "The second graph compares the accuracies of the changing values of m in m-fold croos validations.From the bar plot,it follows that accuracies of 10-fold are around 0.8 and when the number of iterations around 5 and 7 the accuracy becomes lower.Moreover,the operation speed of 2-fold cross validation is faster than that of 10-fold.From the accuracy list,recall list and specificity list of 10-fold,we can observe the changes in accuracy and recall are not very large compared to specificity which has obvious fluctuations on the bar plot.The same goes for 2-fold. Specificity is significantly lower than accuracy and recall and around 0.7 and 0.6.Then,there are six bars in the bar plot which compares 10-fold and 2-fold cross validation.The height of each pair of bars is very similar.The first two bars compare the average accuracy of 2-fold and 10-fold and it shows that the average accuracy of 10-fold is slightly higher than that of 2-fold.The secong 2 bars which compare the average recall and the third 2 bars which compare the average specificity demonstrate that the average recall and specificity of 10-fold and 2-fold are quite similar.In conclusion,the larger m is, the more training data is used each time, the more general the trained model is, and the more reliable the data obtained. But at the same time the speed will be slower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 [4 marks]\n",
    "In `train()`, you are asked to treat the missing value of nominal attributes as a new category. There is another option (as suggested in Thu lecture in week 2): <u>ignoring the missing values</u>. \n",
    "Compare the two methods in both large and small datasets. Comment and explain your observations.\n",
    "You can extract the first 50 records to construct a small dataset.Use Gaussian Naive Bayes only for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer of 150-200 words in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 [4 marks]\n",
    "In week 4, we have learned how to obtain information gain (IG) and gain ratio (GR) to choose an attribute to split a node in a decision tree. We will see how to apply them in the Naive Bayes classification.\n",
    "\n",
    "(a) Compute the GR of each attribute $X_i$, relative to the class distribution. In the Na\\\"ive Bayes classifier, remove attributes in the ascending order of GR: first, remove $P(X_i|c_j)$ such that $X_i$ has the least GR; second, remove $P(X_{i'}|c_j)$ such that $X_{i'}$ has the second least GR,......, until there is only one $X_{i*}$ with the largest GR remaining in the maximand $P(c_j) P(X_{i^*} | c_j)$. Observe the <u>change of the accuracy for both Gaussian and KDE</u> (Choose bandwidth $\\sigma=10$ for KDE).\n",
    "\n",
    "(b) Compute the IG between each pair of attributes. Describe and explain your observations. Choose an attribute and implement an estimator to predict the value of `education num`. Explain why you choose this attribute. Enumerate two other examples that an attribute can be used to estimate the other and explain the reason.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer to **Question 4.a** of 100-150 words in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer to **Question 4.b** of 150-200 words in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authorship Declaration</b>:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: [Hanyue Zhu]\n",
    "   \n",
    "   <b>Dated</b>: [2022/4/8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
